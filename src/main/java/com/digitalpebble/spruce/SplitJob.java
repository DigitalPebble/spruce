// SPDX-License-Identifier: Apache-2.0

package com.digitalpebble.spruce;

import org.apache.commons.cli.*;
import org.apache.spark.api.java.function.FlatMapGroupsFunction;
import org.apache.spark.api.java.function.MapFunction;
import org.apache.spark.sql.*;
import org.apache.spark.sql.catalyst.encoders.RowEncoder;
import scala.Option;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import static com.digitalpebble.spruce.SplitJob.KEY_SUM_USAGE_RATIO;
import static org.apache.spark.sql.functions.lit;
import static org.apache.spark.sql.types.DataTypes.DoubleType;

/**
 * Reads an enriched file containing split line items and give them a share of the impacts of the resources
 * they are related to.
 * Split the impacts generated by SPRUCE by default (operational_energy_kwh, operational_emissions_co2eq_g, embodied_emissions_co2eq_g)
 * but can be overridden using the -c option.
 * New columns are created for each impact with the prefix 'split_'.
 * The input must be enriched CUR reports in Parquet format with the resource IDs and split line items columns.
 **/
public class SplitJob {

    private static final org.slf4j.Logger LOG = org.slf4j.LoggerFactory.getLogger(SplitJob.class);

    public static String split_impact_prefix = "split_";

    public static String KEY_SUM_USAGE_RATIO = "sum_usage_ratio";

    public static String SPLIT_USAGE_RATIO = "split_line_item_split_usage_ratio";

    public static void main(String[] args) {

        final Options options = new Options();
        options.addRequiredOption("i", "input", true, "input path");
        options.addRequiredOption("o", "output", true, "output path");
        options.addOption("c", "columns", true, "comma separated list of column names to use for the impacts");

        String inputPath = null;
        String outputPath = null;
        String impactColumnAsSingleLine = "operational_energy_kwh, operational_emissions_co2eq_g, embodied_emissions_co2eq_g";

        try {
            CommandLineParser parser = new DefaultParser();
            CommandLine cmd = parser.parse(options, args);
            inputPath = cmd.getOptionValue("i");
            outputPath = cmd.getOptionValue("o");
            impactColumnAsSingleLine = cmd.getOptionValue("c", impactColumnAsSingleLine);
        } catch (ParseException e) {
            HelpFormatter formatter = new HelpFormatter();
            formatter.printHelp("SplitJob", options);
            System.exit(1);
        }

        SparkSession spark = SparkSession.builder().appName("SPRUCE-split")//.master("local[*]") // run locally, can step through
                .getOrCreate();

        spark.conf().set("mapreduce.fileoutputcommitter.marksuccessfuljobs", "false");

        Dataset<Row> dataframe = spark.read().parquet(inputPath);
        // csv for now while working on made up example
        // Dataset<Row> dataframe = spark.read().option("header", true).option("mode", "FAILFAST").option("delimiter", ",").option("quote", "\"").option("escape", "\"").csv(inputPath);

        final String[] impactNames = impactColumnAsSingleLine.split(", ?");

        final boolean hasBillingPeriods = !dataframe.schema().getFieldIndex("BILLING_PERIOD").isEmpty();

        Dataset<Row> enriched = enrichSplits(dataframe, impactNames);

        // Write the result as Parquet, with one subdirectory per billing period, similar to the input
        DataFrameWriter<Row> writer = enriched.write().mode("overwrite");

        if (hasBillingPeriods) {
            writer = writer.partitionBy("BILLING_PERIOD");
        }

        // writer.option("header", true).option("delimiter", ",").option("quote", "\"").option("escape", "\"").csv(outputPath);
        writer.parquet(outputPath);

        spark.stop();
    }

    /**
     * Temporary workaround while mucking about with csv inputs
     * A row is a parent if it does not have a parent resource id
     **/
    public static boolean isParent(Row r) {
        String parent_resource_id = r.getAs("split_line_item_parent_resource_id");
        return (parent_resource_id == null || "null".equalsIgnoreCase(parent_resource_id));
    }

    /**
     * Temporary workaround while mucking about with csv inputs
     **/
    public static Double doubleFromColumn(Row row, String columnName) {
        // another csv related problem
        // loaded as string
        int index = row.fieldIndex(columnName);
        Object val = row.get(index);
        double v = 0;
        if (val instanceof Double) {
            return (Double) val;
        }
        if (val instanceof String) {
            // can be null if not set?
            try {
                return Double.parseDouble((String) val);
            } catch (Exception e) {
                return null;
            }
        }
        return null;
    }

    static Dataset<Row> enrichSplits(Dataset<Row> dataframe, final String[] impactNames) {

        final String[] requiredFields = new String[]{"identity_time_interval", "split_line_item_parent_resource_id", "line_item_resource_id", SPLIT_USAGE_RATIO};

        for (String rf : requiredFields) {
            // check that the input contains splits
            boolean missing = dataframe.schema().getFieldIndex(rf).isEmpty();
            if (missing) {
                LOG.error("Input files are missing column {}", rf);
                System.exit(1);
            }
        }

        for (String columnName : impactNames) {
            Option<Object> index = dataframe.schema().getFieldIndex(columnName);
            if (index.isEmpty()) {
                LOG.error("Missing column: '{}'", columnName);
                System.exit(2);
            }
            // create separate columns to avoid double counting
            dataframe = dataframe.withColumn(split_impact_prefix + columnName, lit(null).cast(DoubleType));
        }

        Encoder<Row> encoder = RowEncoder.encoderFor(dataframe.schema());

        KeyValueGroupedDataset<GroupKey, Row> grouped = dataframe.groupByKey((MapFunction<Row, GroupKey>) row -> {
                    String date = row.getAs("identity_time_interval");
                    // COALESCE: use parentID or its own ID if parent
                    String resourceId = row.getAs("split_line_item_parent_resource_id");
                    if (resourceId == null || resourceId.equalsIgnoreCase("null")) {
                        resourceId = row.getAs("line_item_resource_id");
                    }

                    return new GroupKey(date, resourceId);
                }, Encoders.javaSerialization(GroupKey.class) // Encoder for the custom key
        );

        // Step 3: FlatMapGroups to perform custom aggregation
        return grouped.flatMapGroups((FlatMapGroupsFunction<GroupKey, Row, Row>) (key, iterator) -> {

            List<Row> rows = new ArrayList<>();
            iterator.forEachRemaining(rows::add);

            Map<String, Double> agg = ImpactAndUsageRatioAggregator.aggregate(rows, impactNames);

            LOG.info("Group key resource {} date {} - {} rows found", key.getResourceId(), key.getDate(), rows.size());

            // check that aggregated impacts have been found
            boolean noImpactsForParentsInGroup = agg.values().stream().mapToDouble(Double::doubleValue).sum() <= 0;
            if (noImpactsForParentsInGroup) {
                LOG.error("No aggregated impacts found for group {}-{}", key.getResourceId(), key.getDate());
            }

            final double sumUsageRatio = agg.get(KEY_SUM_USAGE_RATIO);

            List<Row> output = new ArrayList<>();
            for (Row row : rows) {
                // no parents impacts? - just write it all out
                if (noImpactsForParentsInGroup) {
                    output.add(row);
                    continue;
                }

                // Only enrich children
                if (isParent(row)) {
                    // parents can go straight out
                    output.add(row);
                    continue;
                }

                // apportion the impacts based on usage ratio
                double ratioUsage = doubleFromColumn(row, SPLIT_USAGE_RATIO);
                double ratioForRow = ratioUsage / sumUsageRatio;

                final Map<String, Object> impactsForRow = new HashMap<>();

                for (String impactName : impactNames) {
                    double val = agg.get(impactName);
                    double localValue = val * ratioForRow;
                    // add with the prefix
                    impactsForRow.put(split_impact_prefix + impactName, localValue);
                }

                output.add(Utils.withUpdatedValues(row, impactsForRow));
            }
            return output.iterator();
        }, encoder);
    }

}

class ImpactAndUsageRatioAggregator {
    public static Map<String, Double> aggregate(List<Row> rows, String[] impactColumns) {
        Map<String, Double> agg = new HashMap<>();

        for (String column : impactColumns) {
            agg.put(column, 0.0);
        }

        agg.put(KEY_SUM_USAGE_RATIO, 0.0);

        for (Row r : rows) {
            // aggregate the impacts if any
            for (String column : impactColumns) {
                final Double v = SplitJob.doubleFromColumn(r, column);
                if (v == null) continue;
                agg.compute(column, (label, value) -> value + v.doubleValue());
            }
            // aggregate the usage ratios if any
            Double ratioUsage = SplitJob.doubleFromColumn(r, SplitJob.SPLIT_USAGE_RATIO);
            if (ratioUsage == null) continue;
            agg.compute(KEY_SUM_USAGE_RATIO, (label, value) -> value + ratioUsage.doubleValue());
        }

        return agg;
    }

}