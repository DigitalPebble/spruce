// SPDX-License-Identifier: Apache-2.0

package com.digitalpebble.spruce;

import org.apache.commons.cli.*;
import org.apache.spark.api.java.function.FlatMapGroupsFunction;
import org.apache.spark.api.java.function.MapFunction;
import org.apache.spark.sql.*;
import org.apache.spark.sql.catalyst.encoders.RowEncoder;
import scala.Option;

import java.io.Serializable;
import java.util.*;

import static com.digitalpebble.spruce.CURColumn.PRODUCT_PRODUCT_FAMILY;
import static org.apache.spark.sql.functions.lit;
import static org.apache.spark.sql.types.DataTypes.DoubleType;

/**
 * Reads an enriched file containing split line items and give them a share of the impacts of the resources
 * they are related to.
 * Split the impacts generated by SPRUCE by default (operational_energy_kwh, operational_emissions_co2eq_g, embodied_emissions_co2eq_g)
 * but can be overridden using the -c option.
 * New columns are created for each impact with the prefix 'split_'.
 **/
public class SplitJob {

    private static final org.slf4j.Logger LOG = org.slf4j.LoggerFactory.getLogger(SplitJob.class);

    public static String split_impact_prefix = "split_";
    public static String ec2_prefix_aggregates = "EC2_";

    public static void main(String[] args) {

        final Options options = new Options();
        options.addRequiredOption("i", "input", true, "input path");
        options.addRequiredOption("o", "output", true, "output path");
        options.addOption("c", "columns", true, "comma separated list of column names to use for the impacts");

        String inputPath = null;
        String outputPath = null;
        String impactColumnAsSingleLine = "operational_energy_kwh, operational_emissions_co2eq_g, embodied_emissions_co2eq_g";

        try {
            CommandLineParser parser = new DefaultParser();
            CommandLine cmd = parser.parse(options, args);
            inputPath = cmd.getOptionValue("i");
            outputPath = cmd.getOptionValue("o");
            impactColumnAsSingleLine = cmd.getOptionValue("c" , impactColumnAsSingleLine);
        } catch (ParseException e) {
            HelpFormatter formatter = new HelpFormatter();
            formatter.printHelp("SplitJob", options);
            System.exit(1);
        }

        SparkSession spark = SparkSession.builder().appName("SPRUCE-split").master("local[*]") // run locally, can step through
                .getOrCreate();

        spark.conf().set("mapreduce.fileoutputcommitter.marksuccessfuljobs", "false");

        // TODO Read the input Parquet file(s)
        // Dataset<Row> dataframe = spark.read().parquet(inputPath);
        // csv for now while working on made up example
        Dataset<Row> dataframe = spark.read().option("header", true).option("mode", "FAILFAST").option("delimiter", ",").option("quote", "\"").option("escape", "\"").csv(inputPath);

        final boolean hasBillingPeriods = !dataframe.schema().getFieldIndex("BILLING_PERIOD").isEmpty();

        // check that the input contains splits
        boolean noSplits = dataframe.schema().getFieldIndex("split_line_item_parent_resource_id").isEmpty();

        if (noSplits) {
            LOG.error("Input files do not have split line items");
            System.exit(1);
        }

        final String[] impactNames = impactColumnAsSingleLine.split(", ?");

        for (String columnName : impactNames) {
            Option<Object> index = dataframe.schema().getFieldIndex(columnName);
            if (index.isEmpty()) {
                LOG.error("Missing column: '{}'", columnName);
                System.exit(2);
            }
            // create separate columns to avoid double counting
            dataframe = dataframe.withColumn(split_impact_prefix + columnName, lit(null).cast(DoubleType));
        }

        Encoder<Row> encoder = RowEncoder.encoderFor(dataframe.schema());

        KeyValueGroupedDataset<ParentAggregator.GroupKey, Row> grouped = dataframe.groupByKey((MapFunction<Row, ParentAggregator.GroupKey>) row -> {
                    String date = row.getAs("identity_time_interval");
                    // COALESCE: if split_line_item_parent_resource_id is null, use identity_line_item_id
                    String resourceId = row.getAs("split_line_item_parent_resource_id");
                    if (resourceId == null || resourceId.equalsIgnoreCase("null")) {
                        resourceId = row.getAs("line_item_resource_id");
                    }

                    return new ParentAggregator.GroupKey(date, resourceId);
                }, Encoders.javaSerialization(ParentAggregator.GroupKey.class) // Encoder for the custom key
        );

        // Step 3: FlatMapGroups to perform custom aggregation
        Dataset<Row> enriched = grouped.flatMapGroups((FlatMapGroupsFunction<ParentAggregator.GroupKey, Row, Row>) (key, iterator) -> {

            List<Row> rows = new ArrayList<>();
            iterator.forEachRemaining(rows::add);

            Map<String, Double> agg = ParentAggregator.aggregate(rows, impactNames);

            LOG.info("Group key resource {} date {} - {} rows found", key.getResourceId(), key.getDate(), rows.size());

            // check that aggregated impacts have been found
            boolean noImpactsForParentsInGroup = agg.values().stream().mapToDouble(Double::doubleValue).sum() <= 0;
            if (noImpactsForParentsInGroup) {
                LOG.error("No aggregated impacts found for group {}-{}", key.getResourceId(), key.getDate());
            }

            // TODO keep track of the total impacts for the
            // splits and make sure they add up with what is found
            // from the parents

            List<Row> output = new ArrayList<>();
            for (Row row : rows) {
                // no parents impacts? - just write it all out
                if (noImpactsForParentsInGroup) {
                    output.add(row);
                    continue;
                }

                // Only enrich children
                if (isParent(row)) {
                    // parents can go straight out
                    output.add(row);
                    continue;
                }

                final double costEC2resource = agg.get("EC2_public_on_demand");

                // the logic is as follows:
                // Work out their share of the EC2 impacts based on ratio of used + unused costs
                // Use splitUsageRatio for their share of non-EC2 impacts

                double split_cost = doubleFromColumn(row, "split_line_item_public_on_demand_split_cost");
                double unused_cost = doubleFromColumn(row, "split_line_item_public_on_demand_unused_cost");
                double ratioUsage = doubleFromColumn(row, "split_line_item_split_usage_ratio");
                double ratioCostForRow = (split_cost + unused_cost) / costEC2resource;

                final Map<String, Object> impactsForRow = new HashMap<>();

                for (String impactName : impactNames){
                    // get value from EC2 e.g. energy in kwh
                    double val = agg.get(ec2_prefix_aggregates +impactName);
                    double localValue = val * ratioCostForRow;
                    // now add any impacts for non-EC2 usage like network
                    val = agg.get(impactName);
                    localValue += val * ratioUsage;
                    // add with the prefix
                    impactsForRow.put(split_impact_prefix+impactName, localValue);
                }

                output.add(Utils.withUpdatedValues(row, impactsForRow));
            }
            return output.iterator();
        }, encoder);


        // Write the result as Parquet, with one subdirectory per billing period, similar to the input
        DataFrameWriter<Row> writer = enriched.write().mode("overwrite");

        if (hasBillingPeriods) {
            writer = writer.partitionBy("BILLING_PERIOD");
        }

        writer.option("header", true).option("delimiter", ",").option("quote", "\"").option("escape", "\"").csv(outputPath);
        // writer.parquet(outputPath);

        spark.stop();
    }

    /**
     * Temporary workaround while mucking about with csv inputs
     * A row is a parent if it does not have a parent resource id
     **/
    public static boolean isParent(Row r) {
        String parent_resource_id = r.getAs("split_line_item_parent_resource_id");
        return (parent_resource_id == null || "null".equalsIgnoreCase(parent_resource_id));
    }

    /**
     * Temporary workaround while mucking about with csv inputs
     **/
    public static Double doubleFromColumn(Row row, String columnName) {
        // another csv related problem
        // loaded as string
        int index = row.fieldIndex(columnName);
        Object val = row.get(index);
        double v = 0;
        if (val instanceof Double) {
            return (Double) val;
        }
        if (val instanceof String) {
            // can be null if not set?
            try {
                return Double.parseDouble((String) val);
            } catch (Exception e) {
                return null;
            }
        }
        return null;
    }

}

class ParentAggregator {

    /**
     * Aggregate parent values for a group
     * There should be at most 1 EC2 instance run and potentially several other items e.g. network
     * Impacts contain energy, and emission estimates
     * Impacts from EC2 resource are kept separate from other resources and prefixed with "EC2_"
     */
    public static Map<String, Double> aggregate(List<Row> rows, String[] impactColumns) {
        Map<String, Double> agg = new HashMap<>();

        for (String column : impactColumns) {
            agg.put(column, 0.0);
            agg.put(SplitJob.ec2_prefix_aggregates + column, 0.0);
        }

        boolean ec2Found = false;

        for (Row r : rows) {
            // parents only
            if (SplitJob.isParent(r)) {
                boolean isEC2 = "Compute Instance".equalsIgnoreCase(PRODUCT_PRODUCT_FAMILY.getString(r));
                if (isEC2) {
                    // crash it for now
                    if (ec2Found) {
                        throw new RuntimeException("More than one EC2 instance found within a group");
                    } else ec2Found = true;
                    // get the cost
                    final Double cost = SplitJob.doubleFromColumn(r, "line_item_unblended_cost");
                    agg.put("EC2_public_on_demand", cost);
                }
                // for each impact
                for (String column : impactColumns) {
                    final Double v = SplitJob.doubleFromColumn(r, column);
                    if (v == null) continue;
                    if (isEC2) column = "EC2_" + column;
                    agg.compute(column, (label, value) -> value + v.doubleValue());
                }
            }
        }

        if (!ec2Found) {
            // TODO log
        }

        return agg;
    }

    public static class GroupKey implements Serializable {
        private final String date;
        private final String resourceId;

        public GroupKey(String date, String resourceId) {
            this.date = date;
            this.resourceId = resourceId;
        }

        public String getDate() {
            return date;
        }

        public String getResourceId() {
            return resourceId;
        }

        @Override
        public boolean equals(Object o) {
            if (this == o) return true;
            if (!(o instanceof GroupKey groupKey)) return false;
            return Objects.equals(date, groupKey.date) && Objects.equals(resourceId, groupKey.resourceId);
        }

        @Override
        public int hashCode() {
            return Objects.hash(date, resourceId);
        }

        @Override
        public String toString() {
            return "GroupKey{" + "date='" + date + '\'' + ", resourceId='" + resourceId + '\'' + '}';
        }
    }
}