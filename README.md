<img src="logo.png" alt="Spruce" width="512"/>

# SPRUCE

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
![Build Status](https://github.com/apache/stormcrawler/actions/workflows/maven.yml/badge.svg)

*Spruce* helps estimate the environmental impact of your cloud usage. By leveraging open source models and data, it enriches
usage reports generated by cloud providers and allows you to build reports and visualisations. Having the greenops and finops data in the same 
place makes it easier to expose your costs and impacts side by side.

*Spruce* uses [Apache Spark](https://spark.apache.org/) to read and write the usage reports (typically in Parquet format) in a scalable way and, thanks to its modular approach, 
splits the enrichment of the data into configurable stages.

A typical sequence of stages would be:
- estimation of embodied emissions from the hardware
- estimation of energy used
- application of PUE and other overheads
- application of carbon intensity factors

Please note that this is currently a prototype which handles only CUR reports from AWS. Not all AWS services are covered.

One of the benefits of using Apache Spark is that you can use [EMR on AWS](https://aws.amazon.com/emr/features/spark/) to enrich 
the CURs at scale without having to export or expose any of your data.

## Prerequisites

You will need to have CUR reports as inputs. Those are generated via [DataExports](https://docs.aws.amazon.com/cur/latest/userguide/what-is-data-exports.html) and stored on S3 as Parquet files.

You need Docker to be installed on your machine in order to run the tests.

## Run Spruce

### With Spark installed

You can copy the Jar from the [latest release](https://github.com/DigitalPebble/spruce/releases) or alternatively, build from source,
which requires Apache Maven and Java 17 or above.

```
mvn clean package
```

To run Spruce locally, you need [Apache Spark](https://spark.apache.org/)  installed  and added to the $PATH:

```
spark-submit --class com.digitalpebble.spruce.SparkJob --driver-memory 8g ./target/spruce-*.jar -i ./curs -o ./output
```

If you downloaded a released jar, make sure the path matches the location of the file.

The `-i` parameter specifies the location of the directory containing the CUR reports in Parquet format.
The `-o` parameter specifies the location of enriched Parquet files generated in output.

The option `-c` allows to specify a JSON configuration file to override the default settings.

## With Docker

Pull the latest Docker image with 

`docker pull ghcr.io/digitalpebble/spruce`

This retrieves a Docker image containing Apache Spark as well as the Spruce jar.

The command below processes the data locally by mounting the directories containing the CURs and output as volumes:
```
docker run -it -v ./curs:/curs -v ./output:/output --rm --name spruce --network host \
ghcr.io/digitalpebble/spruce \
/opt/spark/bin/spark-submit  \
--class com.digitalpebble.spruce.SparkJob \
--driver-memory 4g \
--master 'local[*]' \
/usr/local/lib/spruce.jar \
-i /curs -o /output/enriched
```

## Explore the output

Using [DuckDB](https://duckdb.org/) locally or [Athena](https://docs.aws.amazon.com/athena/latest/ug/what-is.html) on AWS:

```sql
create table enriched_curs as select * from 'output/**/*.parquet';

select line_item_product_code, product_servicecode,
       round(sum(operational_emissions_co2eq_g)/1000,2) as co2_usage_kg,
       round(sum(embodied_emissions_co2eq_g)/1000, 2) as co2_embodied_kg,
       round(sum(energy_usage_kwh),2) as energy_usage_kwh
       from enriched_curs where operational_emissions_co2eq_g > 0.01
       group by line_item_product_code, product_servicecode
       order by co2_usage_kg desc, co2_embodied_kg desc, energy_usage_kwh desc, product_servicecode;
```

should give an output similar to

| line_item_product_code | product_servicecode |      line_item_operation       | co2_usage_kg | energy_usage_kwh | co2_embodied_kg |
|------------------------|---------------------|--------------------------------|-------------:|-----------------:|----------------:|
| AmazonEC2              | AmazonEC2           | RunInstances                   | 538.3        | 1220.14          | 303.41          |
| AmazonECS              | AmazonECS           | FargateTask                    | 181.32       | 399.05           | NULL            |
| AmazonS3               | AmazonS3            | OneZoneIAStorage               | 102.3        | 225.15           | NULL            |
| AmazonS3               | AmazonS3            | GlacierInstantRetrievalStorage | 75.89        | 167.03           | NULL            |
| AmazonEC2              | AmazonEC2           | CreateVolume-Gp3               | 41.63        | 91.62            | NULL            |
| AmazonS3               | AmazonS3            | StandardStorage                | 28.51        | 62.81            | NULL            |
| AmazonDocDB            | AmazonDocDB         | CreateCluster                  | 19.79        | 43.56            | NULL            |
| AmazonECS              | AmazonECS           | ECSTask-EC2                    | 9.26         | 20.37            | NULL            |
| AmazonS3               | AmazonS3            | IntelligentTieringAIAStorage   | 2.33         | 5.13             | NULL            |
| AmazonEC2              | AmazonEC2           | CreateSnapshot                 | 2.31         | 5.82             | NULL            |
| AmazonEC2              | AmazonEC2           | RunInstances:SV001             | 1.79         | 3.94             | 0.78            |
| AmazonS3               | AmazonS3            | StandardIAStorage              | 1.19         | 2.61             | NULL            |
| AmazonS3               | AWSDataTransfer     | GetObjectForRepl               | 1.17         | 2.58             | NULL            |
| AmazonS3               | AWSDataTransfer     | UploadPartForRepl              | 1.01         | 2.22             | NULL            |
| AmazonS3               | AmazonS3            | OneZoneIASizeOverhead          | 0.89         | 1.96             | NULL            |
| AmazonEC2              | AmazonEC2           | CreateVolume-Gp2               | 0.84         | 1.84             | NULL            |
| AmazonEC2              | AWSDataTransfer     | RunInstances                   | 0.18         | 0.39             | NULL            |
| AmazonS3               | AWSDataTransfer     | PutObjectForRepl               | 0.16         | 0.36             | NULL            |
| AmazonS3               | AmazonS3            | DeleteObject                   | 0.16         | 0.35             | NULL            |
| AWSBackup              | AWSBackup           | Storage                        | 0.1          | 0.49             | NULL            |
| AmazonMQ               | AmazonMQ            | CreateBroker:0001              | 0.02         | 0.04             | NULL            |
| AmazonECR              | AWSDataTransfer     | downloadLayer                  | 0.01         | 0.01             | NULL            |
| AmazonS3               | AWSDataTransfer     | PutObject                      | 0.0          | 0.0              | NULL            |

To measure the proportion of the costs for which emissions where calculated

```sql
select
  round(covered * 100 / "total costs", 2) as percentage_costs_covered
from (
  select
    sum(line_item_unblended_cost) as "total costs",
    sum(line_item_unblended_cost) filter (where operational_emissions_co2eq_g is not null) as covered
  from
    enriched_curs
  where
    line_item_line_item_type like '%Usage'
);
```

## License
Licensed under the Apache License, Version 2.0: http://www.apache.org/licenses/LICENSE-2.0

## Contributing
We welcome contributions to the project, see [CONTRIBUTING.md](CONTRIBUTING.md) for instructions on how to do so. Contributions are not only about code: by testing the project on your data, talking about it or asking questions, you will be contributing too!

